# Multi-Armed Bandits

This folder contains implementations and experiments with classic algorithms for the **K-armed bandit problem**.  
We compare three strategies on a 2-armed Bernoulli bandit with means (0.5, 0.5 + Î”):  

- **Explore-Then-Commit (ETC)**  
- **Upper Confidence Bound (UCB1)**  
- **Îµ-Greedy**  

---

## ðŸ“Š Experiments

### Experiment 1: ETC Demo Code
Initial demonstration of the ETC algorithm structure.

---

### Experiment 2: ETC Regret Plot for Different Î”
Regret grows approximately linearly with Î”.  
Because ETC explores for a fixed length `m`, it wastes pulls even when Î” is large.

![ETC](assets/etc.png)

---

### Experiment 3: UCB Regret Plot for Different Î”
Regret **decreases** as Î” increases.  
UCB balances exploration and exploitation adaptively, pulling the optimal arm more often when the reward gap widens.

![UCB](assets/ucb.png)
---

### Experiment 4: Îµ-Greedy Algorithm
Regret **increases** with Î”, since random exploration (`Îµ`) continues throughout and mistakes become costlier when Î” is larger.

![Epsilon-Greedy](assets/epsilon_greedy.png)

---

## ðŸ”¹ Final Comparison

![Comparison](assets/final_comparison.png)

- **UCB** achieves the lowest regret across Î”.  
- **ETC** performs reasonably but worse than UCB.  
- **Îµ-Greedy** suffers from constant exploration, leading to the highest regret.

---

## ðŸ“˜ Conclusion

1. **Regret vs Î”:**  
   - UCB regret decreases with Î” (theoretically optimal trend).  
   - ETC regret increases with Î” due to fixed exploration length.  
   - Îµ-Greedy regret increases with Î” because constant exploration costs scale with Î”.

2. **Best Algorithm:**  
   - **UCB** consistently outperforms ETC and Îµ-Greedy.  
   - ETC is better than Îµ-Greedy but worse than UCB.  
   - Îµ-Greedy is simplest but least efficient.

3. **Takeaway:**  
   UCB1 is the most effective among the tested algorithms, demonstrating the benefit of adaptive exploration in bandit problems.
